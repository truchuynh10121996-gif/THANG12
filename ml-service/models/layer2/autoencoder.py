"""
Autoencoder Model - User Embedding v√† Anomaly Detection
=======================================================
Autoencoder h·ªçc c√°ch n√©n d·ªØ li·ªáu user th√†nh embedding v√† t√°i t·∫°o l·∫°i.
Khi reconstruction error cao = user h√†nh ƒë·ªông b·∫•t th∆∞·ªùng.

∆Øu ƒëi·ªÉm:
- T·∫°o ƒë∆∞·ª£c user embedding h·ªØu √≠ch
- Ph√°t hi·ªán anomaly d·ª±a tr√™n reconstruction error
- Kh√¥ng c·∫ßn labels

Nh∆∞·ª£c ƒëi·ªÉm:
- C·∫ßn tune architecture v√† hyperparameters
- Training c√≥ th·ªÉ kh√¥ng ·ªïn ƒë·ªãnh
"""

import os
import sys
import numpy as np
import pandas as pd
from typing import Dict, Tuple, List, Optional
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
import joblib

# Import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import get_config

config = get_config()

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class AutoencoderNetwork(nn.Module):
    """
    Neural network architecture cho Autoencoder

    Encoder: input_dim -> 32 -> 16 -> 8 (embedding)
    Decoder: 8 -> 16 -> 32 -> input_dim
    """

    def __init__(
        self,
        input_dim: int,
        encoding_dims: List[int] = None
    ):
        """
        Kh·ªüi t·∫°o Autoencoder network

        Args:
            input_dim: S·ªë chi·ªÅu input
            encoding_dims: K√≠ch th∆∞·ªõc c√°c hidden layers [32, 16, 8]
        """
        super(AutoencoderNetwork, self).__init__()

        encoding_dims = encoding_dims or [32, 16, 8]

        # Encoder layers
        encoder_layers = []
        prev_dim = input_dim
        for dim in encoding_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, dim),
                nn.BatchNorm1d(dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = dim

        self.encoder = nn.Sequential(*encoder_layers[:-1])  # Remove last dropout

        # Decoder layers (reverse)
        decoder_layers = []
        decoder_dims = encoding_dims[::-1][1:] + [input_dim]
        prev_dim = encoding_dims[-1]
        for i, dim in enumerate(decoder_dims):
            if i == len(decoder_dims) - 1:
                # Output layer - kh√¥ng c√≥ activation
                decoder_layers.append(nn.Linear(prev_dim, dim))
            else:
                decoder_layers.extend([
                    nn.Linear(prev_dim, dim),
                    nn.BatchNorm1d(dim),
                    nn.ReLU(),
                    nn.Dropout(0.2)
                ])
            prev_dim = dim

        self.decoder = nn.Sequential(*decoder_layers)

        self.embedding_dim = encoding_dims[-1]

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input th√†nh embedding"""
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode embedding th√†nh reconstruction"""
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass

        Returns:
            Tuple (reconstruction, embedding)
        """
        embedding = self.encode(x)
        reconstruction = self.decode(embedding)
        return reconstruction, embedding


class AutoencoderModel:
    """
    Autoencoder model cho user profiling v√† anomaly detection

    S·ª≠ d·ª•ng reconstruction error ƒë·ªÉ ph√°t hi·ªán giao d·ªãch b·∫•t th∆∞·ªùng
    """

    def __init__(self, model_config: Dict = None):
        """
        Kh·ªüi t·∫°o Autoencoder model

        Args:
            model_config: Dict c·∫•u h√¨nh model
        """
        self.config = model_config or config.AUTOENCODER_CONFIG.copy()
        self.model = None
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.threshold = None  # Threshold cho anomaly detection
        self.feature_names = []

        # Training history
        self.history = {'train_loss': [], 'val_loss': []}

    def fit(
        self,
        X: np.ndarray,
        feature_names: List[str] = None,
        validation_split: float = 0.1,
        verbose: bool = True
    ):
        """
        Train Autoencoder

        Args:
            X: Feature matrix (ch·ªâ normal samples n·∫øu c√≥ th·ªÉ)
            feature_names: T√™n features
            validation_split: T·ª∑ l·ªá validation
            verbose: In th√¥ng tin
        """
        if verbose:
            print("[Autoencoder] B·∫Øt ƒë·∫ßu training...")
            print(f"  - S·ªë samples: {X.shape[0]:,}")
            print(f"  - S·ªë features: {X.shape[1]}")
            print(f"  - Device: {device}")

        self.feature_names = feature_names or [f"feature_{i}" for i in range(X.shape[1])]

        # Chu·∫©n h√≥a features
        X_scaled = self.scaler.fit_transform(X)

        # Chia train/validation
        n_samples = len(X_scaled)
        n_val = int(n_samples * validation_split)
        indices = np.random.permutation(n_samples)

        X_train = X_scaled[indices[n_val:]]
        X_val = X_scaled[indices[:n_val]]

        # Convert to tensors
        train_tensor = torch.FloatTensor(X_train)
        val_tensor = torch.FloatTensor(X_val)

        train_loader = DataLoader(
            TensorDataset(train_tensor, train_tensor),
            batch_size=self.config.get('batch_size', 256),
            shuffle=True
        )
        val_loader = DataLoader(
            TensorDataset(val_tensor, val_tensor),
            batch_size=self.config.get('batch_size', 256)
        )

        # Kh·ªüi t·∫°o model
        input_dim = X.shape[1]
        encoding_dims = self.config.get('encoding_dims', [32, 16, 8])

        self.model = AutoencoderNetwork(input_dim, encoding_dims).to(device)

        # Optimizer v√† loss
        optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config.get('learning_rate', 0.001)
        )
        criterion = nn.MSELoss()

        # Training loop
        epochs = self.config.get('epochs', 100)
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0

        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0
            for batch_x, _ in train_loader:
                batch_x = batch_x.to(device)

                optimizer.zero_grad()
                reconstruction, _ = self.model(batch_x)
                loss = criterion(reconstruction, batch_x)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # Validation
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_x, _ in val_loader:
                    batch_x = batch_x.to(device)
                    reconstruction, _ = self.model(batch_x)
                    loss = criterion(reconstruction, batch_x)
                    val_loss += loss.item()

            val_loss /= len(val_loader)

            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    if verbose:
                        print(f"  Early stopping at epoch {epoch + 1}")
                    break

            if verbose and (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch + 1}/{epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}")

        # T√≠nh threshold t·ª´ training data (s·∫Ω ƒë∆∞·ª£c override n·∫øu d√πng fit_with_fraud_optimization)
        self._calculate_threshold(X_scaled)

        self.is_fitted = True
        if verbose:
            print("[Autoencoder] Training ho√†n t·∫•t!")

        # L∆∞u scaled data ƒë·ªÉ c√≥ th·ªÉ optimize threshold sau
        self._last_train_scaled = X_scaled

    def fit_with_fraud_optimization(
        self,
        X_normal: np.ndarray,
        X_fraud: np.ndarray,
        feature_names: List[str] = None,
        validation_split: float = 0.1,
        min_recall: float = 0.6,
        percentile_range: Tuple[int, int] = (80, 98),
        verbose: bool = True
    ) -> Dict:
        """
        Train Autoencoder tr√™n d·ªØ li·ªáu normal v√† t·ªëi ∆∞u threshold v·ªõi d·ªØ li·ªáu fraud.

        ƒê√¢y l√† method khuy·∫øn ngh·ªã cho b√†i to√°n fraud detection ng√¢n h√†ng khi c√≥
        labeled data (bi·∫øt giao d·ªãch n√†o l√† fraud).

        Flow:
        1. Train autoencoder tr√™n d·ªØ li·ªáu normal (h·ªçc pattern b√¨nh th∆∞·ªùng)
        2. T·ªëi ∆∞u threshold b·∫±ng c√°ch duy·ªát percentile 80‚Üí98
        3. Ch·ªçn threshold c√≥ Recall ‚â• min_recall v√† Precision cao nh·∫•t

        Args:
            X_normal: D·ªØ li·ªáu giao d·ªãch b√¨nh th∆∞·ªùng (d√πng ƒë·ªÉ train)
            X_fraud: D·ªØ li·ªáu giao d·ªãch gian l·∫≠n (d√πng ƒë·ªÉ t·ªëi ∆∞u threshold)
            feature_names: T√™n c√°c features
            validation_split: T·ª∑ l·ªá validation (m·∫∑c ƒë·ªãnh 0.1)
            min_recall: Ng∆∞·ª°ng Recall t·ªëi thi·ªÉu (m·∫∑c ƒë·ªãnh 0.6)
            percentile_range: D·∫£i percentile ƒë·ªÉ duy·ªát (m·∫∑c ƒë·ªãnh 80-98)
            verbose: In th√¥ng tin chi ti·∫øt

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ t·ªëi ∆∞u threshold
        """
        if verbose:
            print("\n" + "=" * 80)
            print("üöÄ B·∫ÆT ƒê·∫¶U TRAINING AUTOENCODER V·ªöI T·ªêI ∆ØU THRESHOLD")
            print("=" * 80)
            print(f"\nüìä D·ªØ li·ªáu:")
            print(f"   ‚Ä¢ S·ªë giao d·ªãch normal: {len(X_normal):,}")
            print(f"   ‚Ä¢ S·ªë giao d·ªãch fraud:  {len(X_fraud):,}")
            print(f"   ‚Ä¢ S·ªë features:         {X_normal.shape[1]}")
            print(f"\n‚öôÔ∏è  Tham s·ªë t·ªëi ∆∞u:")
            print(f"   ‚Ä¢ Min Recall:          {min_recall}")
            print(f"   ‚Ä¢ Percentile range:    {percentile_range[0]} ‚Üí {percentile_range[1]}")

        # B∆∞·ªõc 1: Train autoencoder tr√™n normal data
        if verbose:
            print("\n" + "-" * 80)
            print("üìà B∆Ø·ªöC 1: Training Autoencoder tr√™n d·ªØ li·ªáu b√¨nh th∆∞·ªùng")
            print("-" * 80)

        self.fit(
            X=X_normal,
            feature_names=feature_names,
            validation_split=validation_split,
            verbose=verbose
        )

        # B∆∞·ªõc 2: T·ªëi ∆∞u threshold v·ªõi fraud data
        if verbose:
            print("\n" + "-" * 80)
            print("üéØ B∆Ø·ªöC 2: T·ªëi ∆∞u threshold v·ªõi d·ªØ li·ªáu gian l·∫≠n")
            print("-" * 80)

        # Scale fraud data
        X_fraud_scaled = self.scaler.transform(X_fraud)

        # T·ªëi ∆∞u threshold
        optimization_result = self._optimize_threshold_for_recall(
            X_normal=self._last_train_scaled,
            X_fraud=X_fraud_scaled,
            min_recall=min_recall,
            percentile_range=percentile_range,
            verbose=verbose
        )

        if verbose:
            print("\n" + "=" * 80)
            print("‚úÖ TRAINING V√Ä T·ªêI ∆ØU HO√ÄN T·∫§T!")
            print("=" * 80)

        return optimization_result

    def _calculate_threshold(self, X: np.ndarray, percentile: float = None):
        """
        T√≠nh threshold cho anomaly detection

        D·ª±a tr√™n percentile c·ªßa reconstruction error tr√™n training data
        """
        percentile = percentile or self.config.get('threshold_percentile', 95)

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(device)
            reconstruction, _ = self.model(X_tensor)

            # T√≠nh reconstruction error (MSE per sample)
            errors = ((reconstruction - X_tensor) ** 2).mean(dim=1).cpu().numpy()

        self.threshold = np.percentile(errors, percentile)

    def _optimize_threshold_for_recall(
        self,
        X_normal: np.ndarray,
        X_fraud: np.ndarray,
        min_recall: float = 0.6,
        percentile_range: Tuple[int, int] = (80, 98),
        verbose: bool = True
    ) -> Dict:
        """
        T·ªëi ∆∞u threshold ∆∞u ti√™n Recall cho b√†i to√°n fraud detection ng√¢n h√†ng.

        === CHI·∫æN L∆Ø·ª¢C CH·ªåN THRESHOLD ===

        Trong b√†i to√°n ph√°t hi·ªán gian l·∫≠n ng√¢n h√†ng, vi·ªác B·ªé S√ìT giao d·ªãch gian l·∫≠n
        (False Negative) nguy hi·ªÉm h∆°n nhi·ªÅu so v·ªõi c·∫£nh b√°o nh·∫ßm (False Positive):

        - False Negative (b·ªè s√≥t gian l·∫≠n): G√¢y m·∫•t ti·ªÅn th·ª±c s·ª± cho ng√¢n h√†ng v√† kh√°ch h√†ng,
          ·∫£nh h∆∞·ªüng uy t√≠n, c√≥ th·ªÉ d·∫´n ƒë·∫øn ki·ªán t·ª•ng v√† m·∫•t kh√°ch h√†ng.
        - False Positive (c·∫£nh b√°o nh·∫ßm): Ch·ªâ g√¢y b·∫•t ti·ªán t·∫°m th·ªùi, c√≥ th·ªÉ x√°c minh l·∫°i
          qua OTP, g·ªçi ƒëi·ªán x√°c nh·∫≠n, ho·∫∑c y√™u c·∫ßu x√°c th·ª±c b·ªï sung.

        => ∆Øu ti√™n Recall cao (‚â• 0.6) ƒë·ªÉ gi·∫£m b·ªè s√≥t gian l·∫≠n.
        => Trong c√°c threshold ƒë·∫°t Recall, ch·ªçn threshold c√≥ Precision cao nh·∫•t
           ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng c·∫£nh b√°o nh·∫ßm.

        Args:
            X_normal: D·ªØ li·ªáu giao d·ªãch b√¨nh th∆∞·ªùng (ƒë√£ scaled)
            X_fraud: D·ªØ li·ªáu giao d·ªãch gian l·∫≠n (ƒë√£ scaled)
            min_recall: Ng∆∞·ª°ng Recall t·ªëi thi·ªÉu (m·∫∑c ƒë·ªãnh 0.6)
            percentile_range: D·∫£i percentile ƒë·ªÉ duy·ªát (m·∫∑c ƒë·ªãnh 80-98)
            verbose: In b·∫£ng so s√°nh chi ti·∫øt

        Returns:
            Dict ch·ª©a:
            - selected_threshold: Threshold ƒë∆∞·ª£c ch·ªçn
            - selected_percentile: Percentile t∆∞∆°ng ·ª©ng
            - threshold_comparison: B·∫£ng so s√°nh t·∫•t c·∫£ threshold ƒë√£ th·ª≠
            - reason: L√Ω do ch·ªçn threshold n√†y
        """
        from sklearn.metrics import precision_score, recall_score, f1_score

        # T√≠nh reconstruction error cho normal v√† fraud samples
        self.model.eval()
        with torch.no_grad():
            # Normal data errors
            X_normal_tensor = torch.FloatTensor(X_normal).to(device)
            reconstruction_normal, _ = self.model(X_normal_tensor)
            errors_normal = ((reconstruction_normal - X_normal_tensor) ** 2).mean(dim=1).cpu().numpy()

            # Fraud data errors
            X_fraud_tensor = torch.FloatTensor(X_fraud).to(device)
            reconstruction_fraud, _ = self.model(X_fraud_tensor)
            errors_fraud = ((reconstruction_fraud - X_fraud_tensor) ** 2).mean(dim=1).cpu().numpy()

        # T·∫°o labels: 0 = normal, 1 = fraud
        y_true = np.concatenate([
            np.zeros(len(errors_normal)),
            np.ones(len(errors_fraud))
        ])
        all_errors = np.concatenate([errors_normal, errors_fraud])

        # Duy·ªát qua c√°c percentile t·ª´ 80 ƒë·∫øn 98
        percentiles = list(range(percentile_range[0], percentile_range[1] + 1))
        results = []

        for pct in percentiles:
            # Threshold d·ª±a tr√™n percentile c·ªßa normal data
            threshold = np.percentile(errors_normal, pct)

            # D·ª± ƒëo√°n: error > threshold => fraud (1)
            y_pred = (all_errors > threshold).astype(int)

            # T√≠nh metrics
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)

            # ƒê·∫øm s·ªë False Positive v√† False Negative
            fp = np.sum((y_pred == 1) & (y_true == 0))  # Normal b·ªã ƒë√°nh l√† fraud
            fn = np.sum((y_pred == 0) & (y_true == 1))  # Fraud b·ªã b·ªè s√≥t
            tp = np.sum((y_pred == 1) & (y_true == 1))  # Fraud ph√°t hi·ªán ƒë√∫ng
            tn = np.sum((y_pred == 0) & (y_true == 0))  # Normal nh·∫≠n ƒë√∫ng

            results.append({
                'percentile': pct,
                'threshold': threshold,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'true_positive': tp,
                'false_positive': fp,
                'true_negative': tn,
                'false_negative': fn,
                'meets_recall_requirement': recall >= min_recall
            })

        # In b·∫£ng so s√°nh
        if verbose:
            print("\n" + "=" * 100)
            print("üìä B·∫¢NG SO S√ÅNH C√ÅC THRESHOLD (Percentile 80 ‚Üí 98)")
            print("=" * 100)
            print(f"{'Pct':>4} | {'Threshold':>12} | {'Precision':>9} | {'Recall':>8} | {'F1-Score':>8} | {'TP':>5} | {'FP':>5} | {'TN':>5} | {'FN':>5} | {'ƒê·∫°t Recall?':>12}")
            print("-" * 100)

            for r in results:
                meets = "‚úÖ ƒê·∫†T" if r['meets_recall_requirement'] else "‚ùå KH√îNG"
                print(f"{r['percentile']:>4} | {r['threshold']:>12.6f} | {r['precision']:>9.4f} | {r['recall']:>8.4f} | {r['f1_score']:>8.4f} | {r['true_positive']:>5} | {r['false_positive']:>5} | {r['true_negative']:>5} | {r['false_negative']:>5} | {meets:>12}")

            print("-" * 100)

        # L·ªçc c√°c threshold c√≥ Recall >= min_recall
        valid_results = [r for r in results if r['meets_recall_requirement']]

        if len(valid_results) > 0:
            # Ch·ªçn threshold c√≥ Precision cao nh·∫•t trong s·ªë c√°c threshold ƒë·∫°t Recall
            best_result = max(valid_results, key=lambda x: x['precision'])
            reason = (
                f"Ch·ªçn percentile {best_result['percentile']} v√¨:\n"
                f"  1. Recall = {best_result['recall']:.4f} ‚â• {min_recall} (ƒë·∫°t y√™u c·∫ßu t·ªëi thi·ªÉu)\n"
                f"  2. Precision = {best_result['precision']:.4f} (cao nh·∫•t trong c√°c threshold ƒë·∫°t Recall)\n"
                f"  3. Ch·ªâ b·ªè s√≥t {best_result['false_negative']} giao d·ªãch gian l·∫≠n\n"
                f"  4. C·∫£nh b√°o nh·∫ßm {best_result['false_positive']} giao d·ªãch b√¨nh th∆∞·ªùng"
            )
        else:
            # N·∫øu kh√¥ng c√≥ threshold n√†o ƒë·∫°t Recall >= min_recall
            # Ch·ªçn threshold c√≥ Recall cao nh·∫•t
            best_result = max(results, key=lambda x: x['recall'])
            reason = (
                f"‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng c√≥ threshold n√†o ƒë·∫°t Recall ‚â• {min_recall}!\n"
                f"Ch·ªçn percentile {best_result['percentile']} c√≥ Recall cao nh·∫•t = {best_result['recall']:.4f}\n"
                f"ƒê·ªÅ xu·∫•t: Ki·ªÉm tra l·∫°i d·ªØ li·ªáu training ho·∫∑c ƒëi·ªÅu ch·ªânh model architecture."
            )

        # Set threshold ƒë∆∞·ª£c ch·ªçn
        self.threshold = best_result['threshold']
        self.threshold_percentile = best_result['percentile']

        # In k·∫øt qu·∫£ ch·ªçn threshold
        if verbose:
            print("\n" + "=" * 100)
            print("üéØ K·∫æT QU·∫¢ CH·ªåN THRESHOLD T·ªêI ∆ØU CHO FRAUD DETECTION")
            print("=" * 100)
            print(f"\nüìå Threshold ƒë∆∞·ª£c ch·ªçn: {self.threshold:.6f} (Percentile {self.threshold_percentile})")
            print(f"\nüìà Metrics v·ªõi threshold n√†y:")
            print(f"   ‚Ä¢ Precision: {best_result['precision']:.4f}")
            print(f"   ‚Ä¢ Recall:    {best_result['recall']:.4f}")
            print(f"   ‚Ä¢ F1-Score:  {best_result['f1_score']:.4f}")
            print(f"\nüìä Confusion Matrix:")
            print(f"   ‚Ä¢ True Positive (ph√°t hi·ªán ƒë√∫ng fraud):  {best_result['true_positive']}")
            print(f"   ‚Ä¢ False Positive (c·∫£nh b√°o nh·∫ßm):        {best_result['false_positive']}")
            print(f"   ‚Ä¢ True Negative (normal ƒë√∫ng):           {best_result['true_negative']}")
            print(f"   ‚Ä¢ False Negative (b·ªè s√≥t fraud):         {best_result['false_negative']}")
            print(f"\nüí° L√Ω do ch·ªçn threshold n√†y:")
            for line in reason.split('\n'):
                print(f"   {line}")

            print("\n" + "=" * 100)
            print("üìö GI·∫¢I TH√çCH V√å SAO THRESHOLD N√ÄY PH√ô H·ª¢P CHO B√ÄI TO√ÅN NG√ÇN H√ÄNG")
            print("=" * 100)
            print("""
üè¶ TRONG Lƒ®NH V·ª∞C NG√ÇN H√ÄNG, CHI PH√ç C·ª¶A SAI L·∫¶M KH√îNG ƒê·ªêI X·ª®NG:

   ‚ùå False Negative (B·ªè s√≥t gian l·∫≠n) - CHI PH√ç R·∫§T CAO:
      ‚Ä¢ M·∫•t ti·ªÅn th·ª±c s·ª± c·ªßa ng√¢n h√†ng/kh√°ch h√†ng
      ‚Ä¢ T·ªïn th·∫•t t√†i ch√≠nh tr·ª±c ti·∫øp (c√≥ th·ªÉ h√†ng t·ª∑ ƒë·ªìng)
      ‚Ä¢ ·∫¢nh h∆∞·ªüng uy t√≠n ng√¢n h√†ng nghi√™m tr·ªçng
      ‚Ä¢ Kh√°ch h√†ng m·∫•t ni·ªÅm tin, r·ªùi b·ªè ng√¢n h√†ng
      ‚Ä¢ C√≥ th·ªÉ d·∫´n ƒë·∫øn ki·ªán t·ª•ng ph√°p l√Ω

   ‚ö†Ô∏è  False Positive (C·∫£nh b√°o nh·∫ßm) - CHI PH√ç TH·∫§P:
      ‚Ä¢ Ch·ªâ g√¢y b·∫•t ti·ªán t·∫°m th·ªùi cho kh√°ch h√†ng
      ‚Ä¢ C√≥ th·ªÉ x√°c minh qua: OTP, g·ªçi ƒëi·ªán, x√°c th·ª±c sinh tr·∫Øc h·ªçc
      ‚Ä¢ Kh√°ch h√†ng th∆∞·ªùng th√¥ng c·∫£m khi hi·ªÉu ƒë√¢y l√† bi·ªán ph√°p b·∫£o v·ªá
      ‚Ä¢ Chi ph√≠ v·∫≠n h√†nh tƒÉng nh·∫π (th√™m nh√¢n vi√™n x√°c minh)

üìä CHI·∫æN L∆Ø·ª¢C THRESHOLD ƒê√É √ÅP D·ª§NG:

   1. ∆Øu ti√™n Recall ‚â• 0.6:
      ‚Üí ƒê·∫£m b·∫£o ph√°t hi·ªán √≠t nh·∫•t 60% giao d·ªãch gian l·∫≠n
      ‚Üí Gi·∫£m thi·ªÉu t·ªïn th·∫•t t√†i ch√≠nh do b·ªè s√≥t

   2. Trong c√°c threshold ƒë·∫°t Recall, ch·ªçn Precision cao nh·∫•t:
      ‚Üí Gi·∫£m s·ªë l∆∞·ª£ng c·∫£nh b√°o nh·∫ßm
      ‚Üí T·ªëi ∆∞u tr·∫£i nghi·ªám kh√°ch h√†ng
      ‚Üí Gi·∫£m chi ph√≠ v·∫≠n h√†nh x√°c minh

   3. Duy·ªát percentile 80‚Üí98 thay v√¨ c·ªë ƒë·ªãnh 95:
      ‚Üí Linh ho·∫°t theo ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu
      ‚Üí T√¨m ƒëi·ªÉm c√¢n b·∫±ng t·ªëi ∆∞u gi·ªØa Recall v√† Precision

‚öñÔ∏è  C√ÇN B·∫∞NG GI·ªÆA AN TO√ÄN V√Ä TR·∫¢I NGHI·ªÜM NG∆Ø·ªúI D√ôNG:

   ‚Ä¢ Threshold th·∫•p h∆°n (percentile nh·ªè) ‚Üí Nhi·ªÅu c·∫£nh b√°o h∆°n ‚Üí Recall cao h∆°n
   ‚Ä¢ Threshold cao h∆°n (percentile l·ªõn) ‚Üí √çt c·∫£nh b√°o h∆°n ‚Üí Precision cao h∆°n

   ‚Üí Chi·∫øn l∆∞·ª£c n√†y t√¨m ƒëi·ªÉm c√¢n b·∫±ng t·ªëi ∆∞u!
""")
            print("=" * 100)

        return {
            'selected_threshold': float(self.threshold),
            'selected_percentile': self.threshold_percentile,
            'threshold_comparison': results,
            'reason': reason,
            'best_metrics': {
                'precision': best_result['precision'],
                'recall': best_result['recall'],
                'f1_score': best_result['f1_score']
            }
        }

    def get_reconstruction_error(self, X: np.ndarray) -> np.ndarray:
        """
        T√≠nh reconstruction error cho m·ªói sample

        Args:
            X: Feature matrix

        Returns:
            np.ndarray: Reconstruction error cho m·ªói sample
        """
        if not self.is_fitted:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train!")

        X_scaled = self.scaler.transform(X)

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_scaled).to(device)
            reconstruction, _ = self.model(X_tensor)

            errors = ((reconstruction - X_tensor) ** 2).mean(dim=1).cpu().numpy()

        return errors

    def get_embedding(self, X: np.ndarray) -> np.ndarray:
        """
        L·∫•y embedding cho c√°c samples

        Args:
            X: Feature matrix

        Returns:
            np.ndarray: Embedding vectors
        """
        if not self.is_fitted:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train!")

        X_scaled = self.scaler.transform(X)

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_scaled).to(device)
            _, embedding = self.model(X_tensor)

        return embedding.cpu().numpy()

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        D·ª± ƒëo√°n anomaly

        Args:
            X: Feature matrix

        Returns:
            np.ndarray: 0 = normal, 1 = anomaly/fraud
        """
        errors = self.get_reconstruction_error(X)
        return (errors > self.threshold).astype(int)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        T√≠nh x√°c su·∫•t anomaly/fraud

        Chuy·ªÉn ƒë·ªïi reconstruction error th√†nh x√°c su·∫•t

        Args:
            X: Feature matrix

        Returns:
            np.ndarray: X√°c su·∫•t fraud
        """
        errors = self.get_reconstruction_error(X)

        # Normalize errors to [0, 1]
        # S·ª≠ d·ª•ng sigmoid v·ªõi threshold l√†m center
        proba = 1 / (1 + np.exp(-(errors - self.threshold) / (self.threshold + 1e-8)))

        return np.clip(proba, 0, 1)

    def evaluate(
        self,
        X: np.ndarray,
        y_true: np.ndarray,
        verbose: bool = True
    ) -> Dict:
        """
        ƒê√°nh gi√° model

        Args:
            X: Feature matrix
            y_true: Labels th·ª±c t·∫ø
            verbose: In k·∫øt qu·∫£

        Returns:
            Dict ch·ª©a metrics
        """
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score,
            f1_score, roc_auc_score, confusion_matrix
        )

        y_pred = self.predict(X)
        y_proba = self.predict_proba(X)

        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_true, y_proba),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
            'threshold': float(self.threshold)
        }

        if verbose:
            print("\n[Autoencoder] K·∫øt qu·∫£ ƒë√°nh gi√°:")
            print(f"  - Accuracy:  {metrics['accuracy']:.4f}")
            print(f"  - Precision: {metrics['precision']:.4f}")
            print(f"  - Recall:    {metrics['recall']:.4f}")
            print(f"  - F1-Score:  {metrics['f1_score']:.4f}")
            print(f"  - ROC-AUC:   {metrics['roc_auc']:.4f}")
            print(f"  - Threshold: {metrics['threshold']:.6f}")

        return metrics

    def save(self, path: str = None):
        """
        L∆∞u model

        Args:
            path: ƒê∆∞·ªùng d·∫´n file
        """
        if path is None:
            path = os.path.join(config.SAVED_MODELS_DIR, 'autoencoder.pth')

        os.makedirs(os.path.dirname(path), exist_ok=True)

        save_data = {
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'config': self.config,
            'threshold': self.threshold,
            'feature_names': self.feature_names,
            'history': self.history,
            'is_fitted': self.is_fitted
        }

        torch.save(save_data, path)
        print(f"[Autoencoder] ƒê√£ l∆∞u model: {path}")

    def load(self, path: str = None):
        """
        Load model

        Args:
            path: ƒê∆∞·ªùng d·∫´n file
        """
        if path is None:
            path = os.path.join(config.SAVED_MODELS_DIR, 'autoencoder.pth')

        if not os.path.exists(path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {path}")

        save_data = torch.load(path, map_location=device)

        self.scaler = save_data['scaler']
        self.config = save_data['config']
        self.threshold = save_data['threshold']
        self.feature_names = save_data['feature_names']
        self.history = save_data['history']
        self.is_fitted = save_data['is_fitted']

        # Rebuild model
        input_dim = len(self.feature_names)
        encoding_dims = self.config.get('encoding_dims', [32, 16, 8])
        self.model = AutoencoderNetwork(input_dim, encoding_dims).to(device)
        self.model.load_state_dict(save_data['model_state_dict'])

        print(f"[Autoencoder] ƒê√£ load model: {path}")

    def explain_prediction(
        self,
        X: np.ndarray,
        sample_idx: int = 0
    ) -> Dict:
        """
        Gi·∫£i th√≠ch d·ª± ƒëo√°n

        Args:
            X: Feature matrix
            sample_idx: Index c·ªßa sample

        Returns:
            Dict gi·∫£i th√≠ch
        """
        sample = X[sample_idx:sample_idx + 1]
        X_scaled = self.scaler.transform(sample)

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_scaled).to(device)
            reconstruction, embedding = self.model(X_tensor)

            # Error per feature
            feature_errors = ((reconstruction - X_tensor) ** 2).squeeze().cpu().numpy()
            total_error = feature_errors.mean()

        # Top features v·ªõi error cao nh·∫•t
        feature_error_dict = dict(zip(self.feature_names, feature_errors))
        feature_error_dict = dict(sorted(
            feature_error_dict.items(),
            key=lambda x: x[1],
            reverse=True
        ))

        return {
            'reconstruction_error': float(total_error),
            'threshold': float(self.threshold),
            'is_anomaly': bool(total_error > self.threshold),
            'fraud_probability': float(self.predict_proba(sample)[0]),
            'embedding': embedding.squeeze().cpu().numpy().tolist(),
            'feature_errors': feature_error_dict,
            'top_anomalous_features': list(feature_error_dict.keys())[:5]
        }
